{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac3f487-a9c2-4d4b-b656-a0ceb5bec5aa",
   "metadata": {},
   "source": [
    "# Video Generation with ComfyUI on AMD Instinct MI300X GPU\n",
    "\n",
    "With the rapid development of Artificial Intelligence Generated Content (AIGC) technologies, text-to-video and image-to-video generation are becoming powerful tools for creators, researchers, and media professionals.\n",
    "\n",
    "[ComfyUI](https://github.com/comfyanonymous/ComfyUI) is a node-based graphical interface designed for diffusion models, enabling users to visually construct AI image/video generation workflows through modular operations. Its flexible node design, efficiency, and compatibility make it an excellent choice for boosting productivity in creative workflows.\n",
    "\n",
    "This hands-on workshop walks you through setting up and running ComfyUI on **AMD Instinct MI300X GPUs using ROCm software**. You will learn how to configure your environment, install ComfyUI, and generate videos from text or a combination of text and images on AMD's latest data center GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c3ac2-4840-470b-9f49-d8f7199be87d",
   "metadata": {},
   "source": [
    "## Prerequisites  \n",
    "This workshop was developed and tested with the following setup:  \n",
    "\n",
    "### Hardware  \n",
    "- **AMD Instinct MI300X GPUs**:  We're using AMD Dev Cloud with Instinct MI300X hardware for this workshop.  \n",
    "\n",
    "### Software  \n",
    "- **ROCm 7.0**: Confirm the ROCm with running:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31e564-df05-4a2c-9708-9485e2c72440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1943a-d63f-4a0a-90c2-56b6ed17a54f",
   "metadata": {},
   "source": [
    "This command will list your available AMD GPUs along with their key details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b853059-2cd9-436e-8eb7-1966af346b6a",
   "metadata": {},
   "source": [
    "# ComfyUI setup\n",
    "\n",
    "To set up the ComfyUI inference environment, follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc13c3c-c701-4ad1-98b8-85be6f5b86a1",
   "metadata": {},
   "source": [
    "#### Verify the PyTorch installation\n",
    "\n",
    "Verify that PyTorch is correctly installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed37687-f9ea-4089-82c7-0633e6aed7e0",
   "metadata": {},
   "source": [
    "**Step 1** Verify PyTorch is installed and can detect the GPU compute device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b280f81-ff82-426f-99c8-e067a8772fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c 'import torch' 2> /dev/null && echo 'Success' || echo 'Failure'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6055b-345d-454f-a1f6-b4a2d71ae354",
   "metadata": {},
   "source": [
    "The expected result is `Success`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad2a06-0e52-4567-87b9-b87879e0a44a",
   "metadata": {},
   "source": [
    "**Step 2** Confirm the GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991da4d1-3348-4d96-b819-52ce6f4cccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c 'import torch; print(torch.cuda.is_available())'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5b3e0-dcaf-4615-90cb-d851532db399",
   "metadata": {},
   "source": [
    "The expected result is `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14c645-a8fe-44b6-abc1-38f535e69b78",
   "metadata": {},
   "source": [
    "**Step 3** Display the installed GPU device name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b504353-0b6c-4ead-b8d9-3d7032d5601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c \"import torch; print(f'device name [0]:', torch.cuda.get_device_name(0))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3737d18a-c9a7-4698-8642-a33378d61672",
   "metadata": {},
   "source": [
    "The expected result should be similar to: `device name [0]:  AMD Instinct MI300X`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a755f05b-7af1-498c-8590-c895267290fc",
   "metadata": {},
   "source": [
    "# ComfyUI installation\n",
    "\n",
    "Install ComfyUI from source on the system with the AMD GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2782b3-ab62-400d-99e3-069a79ecc216",
   "metadata": {},
   "source": [
    "Go to the following repo and Ensure that PyTorch will not be reinstalled with the CUDA version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca6951-6aa1-4090-84de-00e356af127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ComfyUI\n",
    "!sed -i.bak -E '/^(torch|torchaudio|torchvision)([<>=~!0-9.]*)?$/s/^/# /' requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b574a73-8019-44d1-811b-ffeb4b435c6a",
   "metadata": {},
   "source": [
    "Install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9551f-91d8-489d-8f40-7ae7848ad601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f7214-8404-4f59-b33a-a2a57398d70d",
   "metadata": {},
   "source": [
    "# Running video generation\n",
    "\n",
    "Follow these steps to generate video from your text or text and image\n",
    "\n",
    "---\n",
    "\n",
    "## Model Setup\n",
    "\n",
    "### 📦 Required Models\n",
    "\n",
    "We’ve already downloaded the necessary models and placed them in the correct ComfyUI directories:\n",
    "\n",
    "- **Diffusion Models**:  \n",
    "  - wan2.2_i2v_high_noise_14B_fp16.safetensors  \n",
    "  - wan2.2_i2v_low_noise_14B_fp16.safetensors\n",
    "  - qwen_image_edit_2509_bf16.safetensors\n",
    "\n",
    "- **VAE Model**:  \n",
    "  - wan_2.1_vae.safetensors\n",
    "  - qwen_image_vae.safetensors  \n",
    "\n",
    "- **Text Encoder Model**:  \n",
    "  - umt5_xxl_fp16.safetensors\n",
    "  - qwen_2.5_vl_7b.safetensors\n",
    "\n",
    "- **LoRAs**:  \n",
    "  - wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors  \n",
    "  - wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\n",
    "  - Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors\n",
    "\n",
    "---\n",
    "\n",
    "### 📂 Directory Structure\n",
    "\n",
    "The models are organized inside the `ComfyUI/models` folder as follows:\n",
    "\n",
    "```text\n",
    "ComfyUI/\n",
    "└───📂 models/\n",
    "    ├───📂 diffusion_models/\n",
    "    │   ├─── wan2.2_i2v_high_noise_14B_fp16.safetensors\n",
    "    │   └─── wan2.2_i2v_low_noise_14B_fp16.safetensors\n",
    "    │   └─── qwen_image_edit_2509_bf16.safetensors\n",
    "    ├───📂 text_encoders/\n",
    "    │   └─── umt5_xxl_fp16.safetensors\n",
    "    │   └─── qwen_2.5_vl_7b.safetensors\n",
    "    ├───📂 vae/\n",
    "    │   └─── wan_2.1_vae.safetensors\n",
    "    │   └─── qwen_image_vae.safetensors \n",
    "    └───📂 loras/\n",
    "    │   ├─── put_loras_here\n",
    "    │   └─── wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \n",
    "    │   └─── wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\n",
    "    │   └─── Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde045f7-2a93-4432-a391-454677ffd110",
   "metadata": {},
   "source": [
    "## 🚀 Launch the Server\n",
    "\n",
    "To start the ComfyUI server, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b68b77-2808-4bab-922f-c2110d0d9345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 main.py --listen 0.0.0.0 --port 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805ba6d-8f00-4bbd-91c9-7695c6b2805a",
   "metadata": {},
   "source": [
    "Once you see a message like `To see the GUI go to: http://0.0.0.0:9000` in the terminal output, it means the **ComfyUI server has launched successfully**.  \n",
    "\n",
    "⚠️  **Do not** use `http://0.0.0.0:9000` directly — instead, use the **Your App URL** shown on the **Launch Notebook** page to access the interface in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16f901-bc27-468d-bbcf-b9df99734c1f",
   "metadata": {},
   "source": [
    "## 🌐 Open the ComfyUI Interface\n",
    "\n",
    "Once you paste the corrected link into your browser, the ComfyUI interface will open.  \n",
    "\n",
    "You’ll see:  \n",
    "- A **node-based canvas** in the main area  \n",
    "- A **sidebar on the left**, where you can load workflows and start generating  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Load the Workflow  \n",
    "\n",
    "ComfyUI workflows define the full pipeline and all parameters required to generate an image or video. These workflows are typically saved as a JSON file or embedded within an animated WebP image (`*.webp`). You can create your own workflow from scratch or customize one provided by third parties.  \n",
    "\n",
    "In this workshop, we’ll use the **multimodel_workflow.json** template.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 How to Load the Workflow  \n",
    "\n",
    "Once you've launched the ComfyUI interface in your browser:  \n",
    "\n",
    "1. Click **Workflows** in the sidebar.  \n",
    "2. Select **multimodel_workflow.json**.  \n",
    "\n",
    "The full workflow, with all pre-configured nodes, will load onto your canvas and be ready to run.  \n",
    "\n",
    "This setup ensures you can immediately start experimenting with **multimodel video generation** using the **Wan 2.2 14B model** and the **Qwen-Image model**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42bd00-94bd-4da5-b6d2-d707a2d45c7a",
   "metadata": {},
   "source": [
    "## ▶️ Run the Workflow\n",
    "\n",
    "Before running the video generation, make sure the correct models and settings are loaded\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Execute Video Generation\n",
    "\n",
    "Click the **Run** button, or press **Ctrl (Cmd) + Enter** to start the video generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c0f16-994b-488f-93a9-6f2fe5bf87b8",
   "metadata": {},
   "source": [
    "![ComfyUI Interface](./assets/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b2b17-cacf-42e7-a8ce-ae3068b594bf",
   "metadata": {},
   "source": [
    "## 🧠 Advanced Assignment: Create an AMD-Themed Video in ComfyUI\n",
    "\n",
    "### 🎯 Objective\n",
    "Your task is to create a short, cinematic video inspired by **AMD’s technological vision**, using a customized workflow and prompts of your choice.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Instructions\n",
    "\n",
    "1. **Choose Your Video Model**\n",
    "\n",
    "   You are free to choose **any video generation model** supported in ComfyUI (e.g., `wan2.2`, `LTX-Video`, etc.).\n",
    "\n",
    "   > 💡 **Tip**: If you want to use another template — for example the **Wan 2.2 14B Fun Camera Control** — you can download the required models and place them in the correct directories just like the template shows.  \n",
    "   > For example, in Jupyter Notebook, run:\n",
    "   > ```bash\n",
    "   > !wget https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors \\\n",
    "   >   -O models/diffusion_models/wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors\n",
    "   > ```\n",
    "\n",
    "2. **Design Your Workflow**\n",
    "\n",
    "   - You may start from an existing workflow template or build your own from scratch.\n",
    "\n",
    "3. **Set Your Parameters**\n",
    "   - Adjust resolution and number of frames (e.g., 720x720, 24 frames)\n",
    "   - Tweak prompts, model settings, or sampler parameters to shape the style\n",
    "\n",
    "---\n",
    "\n",
    "### 🎬 Creative Task: *“The AMD Vision”*\n",
    "\n",
    "Create a **5-10 second video** with one of the following AMD-related themes:\n",
    "\n",
    "- 🧠 *AI on AMD Instinct* — visualize powerful AI inference or training on MI300X\n",
    "- ⚙️ *Next-Gen Compute* — represent raw compute performance, silicon, or futuristic chips\n",
    "- 🔥 *Gaming with Radeon* — imagine energy, motion, or light inspired by GPU gaming\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Requirements\n",
    "- Export as **`.mp4`** and save your **workflow `.json`**\n",
    "- Include a **short description** of your concept, model choice, and prompt design\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Bonus Ideas\n",
    "\n",
    "- Start from an uploaded image (e.g., AMD product photo)\n",
    "\n",
    "---\n",
    "\n",
    "### 📤 Submission\n",
    "\n",
    "Please submit the following:\n",
    "- 🎞️ Your generated video (`.mp4`)\n",
    "- 🧩 Your ComfyUI workflow (`.json`)\n",
    "- 📝 A short write-up on your approach and prompt strategy\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Get Creative\n",
    "\n",
    "Feel free to explore and experiment — the goal is to combine **technical skill** and **creative storytelling** using video generation models powered by AMD hardware.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
